{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_clean #this file does the preprocessing in the \"Preparing the Data\" section below. I threw it all in a function so I could easily perform the same steps on the holdout_.csv data as the training data.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the \"Created Date Time\" variable, I added a variable corresponding to month and day of week. I also used zip code median and mean income (the mean income came from a dataset I got from the IRS SOI database; the median income I scraped from http://www.incomebyzipcode.com). I discarded the variables \"Unnamed: 0,\" \"System ID,\" and \"Applicant City.\" To deal with missing values, for the categorical variables, I simply treated missing observations as another category; for the inherently continuous variables (i.e., email, birthdate, and the income variables) I replaced missing observations with the median value. In order to get the holdout data to look like the preprocessed training data (because of encoding issues) I, first, changed the 'Birthdate' variable from a date into days, and, second, appeneded the 35,000 or so \"holdout\" observations to the end of the training data, then preprocessed everything together, and then separated the datasets again. To generate the predictions I did a (simple) cross-validation grid search using a random forest model. A description of the key features shown in the feature importance diagram is provided below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included in the folder are the following files...\n",
    "* this notebook\n",
    "* a python files with all of the below (I don't like working in notebooks usually) (brent_travis_howe_svg_ass_2017.01.13.py and data_clean.py)\n",
    "* the python file used to scrape the income data (zipwho_scrape.py)\n",
    "* the median income data set (income_by_zipcode.txt; the mean income dataset was too big...if you search for the file (14zpallagi.csv) online it will come up)\n",
    "* and the feature importances image (feature_importances.png)\n",
    "* the holdout\\_.csv file with my predictions (holdout_predict.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/Users/brenthowe/datascience/data sets/svg/train_test_.csv')\n",
    "le = LabelEncoder()\n",
    "\n",
    "df1['target'].fillna(value=0, inplace=True)\n",
    "\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(path2)\n",
    "df2['Birthdate'] = df2['Birthdate'].astype(str).apply(lambda x: x[:-2] + '19' + x[-2:] if ((x!='nan') and (x[-2:]!='00')) else x)\n",
    "df2['Birthdate'] = df2['Birthdate'].astype(str).apply(lambda x: x[:-2] + '20' + x[-2:] if ((x!='nan') and (x[-2:]=='00')) else x)\n",
    "df2['Birthdate'] = pd.to_datetime(df2['Birthdate'])\n",
    "df2['Birthdate'] = (pd.to_datetime('2017/01/17') - df2['Birthdate']).dt.days\n",
    "df2['Birthdate'].fillna(value=df2['Birthdate'].median(), inplace=True)\n",
    "\n",
    "\n",
    "df = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print pd.DataFrame(df['Unnamed: 0'].unique()).describe() # This looks like a unique identifier\n",
    "# print pd.DataFrame(df['System ID'].unique()).describe() # 137042 unique values here...likely doesn't carry any useful information\n",
    "\n",
    "df.drop(['Unnamed: 0', 'System ID'], 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assume Created Date Time is correlated with purchasing opportunity day\n",
    "\n",
    "df['Created Date Time'] = pd.to_datetime(df['Created Date Time']) # convert to pandas datetime object\n",
    "df['day_of_week'] = df['Created Date Time'].dt.dayofweek # create day of week variable\n",
    "df['month'] = df['Created Date Time'].dt.month # create month variable\n",
    "\n",
    "# no major holidays in dataset time frame so won't create a holiday dummy\n",
    "\n",
    "df.drop(['Created Date Time'], 1, inplace=True)\n",
    "\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print pd.DataFrame(df['Neustar Result Code'].unique()).describe() #6 unique values\n",
    "# print df['Neustar Result Code'].unique()\n",
    "\n",
    "df['Neustar Result Code'] = df['Neustar Result Code']\n",
    "# df.ix[df['Neustar Result Code'].isnull(), 'Neustar Result Code'] = -1\n",
    "df['Neustar Result Code'].fillna(value=999, inplace=True)\n",
    "\n",
    "# print df['Neustar Result Code'].unique()\n",
    "\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print pd.DataFrame(df['Lead Source'].unique()).describe() #183 unique values\n",
    "\n",
    "le.fit(df['Lead Source'])\n",
    "df['Lead Source'] = le.transform(df['Lead Source']) \n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print df['Smoker'].unique()\n",
    "\n",
    "df.ix[df['Smoker']=='FALSE', 'Smoker'] = 'No'\n",
    "df.ix[df['Smoker']=='OE State (Not Required)', 'Smoker'] = np.NaN\n",
    "df.ix[df['Smoker']=='N', 'Smoker'] = 'No'\n",
    "df.ix[df['Smoker']=='1', 'Smoker'] = 'Yes'\n",
    "df.ix[df['Smoker']=='TZT.Leads.Runtime.Domain.Models.Field', 'Smoker'] = np.NaN\n",
    "df.ix[df['Smoker']=='TRUE', 'Smoker'] = 'Yes'\n",
    "df.ix[df['Smoker']=='Y', 'Smoker'] = 'Yes'\n",
    "\n",
    "df['Smoker'].fillna(value='0', inplace=True)\n",
    "\n",
    "le.fit(df['Smoker'])\n",
    "df['Smoker'] = le.transform(df['Smoker']) \n",
    "\n",
    "# print df['Smoker'].describe()\n",
    "\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print df['Emails'].unique()\n",
    "\n",
    "df['Emails'].fillna(value=df['Emails'].median(), inplace=True) #use median because the distribution is skewed\n",
    "\n",
    "le.fit(df['Emails'])\n",
    "df['Emails'] = le.transform(df['Emails']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looks like birthdate may be age in days, so I'll leave it as it is.\n",
    "\n",
    "# Use median value in place of a missing value\n",
    "df['Birthdate'].fillna(value=df['Birthdate'].median(), inplace=True) #use median because the distribution is skewed\n",
    "\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print df['Gender'].unique()\n",
    "\n",
    "df.ix[df['Gender']=='F', 'Gender'] = 'Female'\n",
    "df.ix[df['Gender']=='M', 'Gender'] = 'Male'\n",
    "\n",
    "df['Gender'].fillna(value='0', inplace=True)\n",
    "\n",
    "le.fit(df['Gender'])\n",
    "df['Gender'] = le.transform(df['Gender']) \n",
    "\n",
    "# print df['Gender'].describe()\n",
    "\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print df['Applicant State/Province'].unique()\n",
    "\n",
    "df['Applicant State/Province'].fillna(value='0', inplace=True)\n",
    "\n",
    "le.fit(df['Applicant State/Province'])\n",
    "df['Applicant State/Province'] = le.transform(df['Applicant State/Province'])\n",
    "\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['l'] = df['Applicant Zip/Postal Code'].astype(str).apply(lambda x: len(x))\n",
    "df.ix[(df['l']>5) & (df['l']<10), 'Applicant Zip/Postal Code'] = np.NaN\n",
    "\n",
    "df['Applicant Zip/Postal Code'] = df['Applicant Zip/Postal Code'].astype(str).apply(lambda x: x.split('-')[0] if len(x)>5 else x)\n",
    "df['Applicant Zip/Postal Code'] = df['Applicant Zip/Postal Code'].astype(str).apply(lambda x: '0' + x if len(x)==4 else x)\n",
    "\n",
    "# d = df[df['l']==3]\n",
    "# print d['Applicant Zip/Postal Code'].unique()\n",
    "\n",
    "df['Applicant Zip/Postal Code'] = df['Applicant Zip/Postal Code'].astype(str).apply(lambda x: '00' + x if (len(x)==3) & (x!='nan') else x)\n",
    "\n",
    "# df['Applicant Zip/Postal Code'] = df['Applicant Zip/Postal Code'].astype(float)\n",
    "# df.ix[df['Applicant Zip/Postal Code'].isnull(), 'Applicant Zip/Postal Code'] = -1\n",
    "\n",
    "df['zip'] = df['Applicant Zip/Postal Code']\n",
    "\n",
    "df.drop(['l', 'Applicant Zip/Postal Code'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zip_average_income():\n",
    "    df = pd.read_csv('/Users/brenthowe/datascience/galvanize/project/data/14zpallagi.csv')\n",
    "    df_sum = df.groupby('zipcode')[['N02650', 'A02650']].sum()\n",
    "    df_sum['mean_income'] = df_sum['A02650']/df_sum['N02650']\n",
    "    df_sum.drop(['N02650', 'A02650'], 1, inplace=True)\n",
    "    df_sum['zip'] = df_sum.index.astype('str')\n",
    "    df_sum['zip'] = df_sum.zip.apply(lambda x: x.zfill(5))\n",
    "    df_sum.set_index('zip', inplace=True)\n",
    "\n",
    "    us_income = float(df_sum.loc['00000'].values)\n",
    "    df_sum.drop(df_sum.index[0], inplace=True)\n",
    "    df_sum['zip'] = df_sum.index\n",
    "    return us_income, df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "us_income, df_sum = zip_average_income()\n",
    "\n",
    "df_sum['mean_income'] = df_sum['mean_income']*1000\n",
    "df = df.merge(df_sum, how='left', on='zip')\n",
    "\n",
    "print df['mean_income'].mean()\n",
    "df['mean_income'].fillna(value=df['mean_income'].mean(), inplace=True) #approximate mean income in the United States\n",
    "\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_median = pd.read_csv('income_by_zipcode.txt', sep='\\t', index_col=False)\n",
    "df_median['zip'] = df_median['zipcode, median_income'].str[:5]\n",
    "df_median['zip'] = df_median['zip'].apply(lambda x: '0'+ x if len(x)==4 else x)\n",
    "df_median['median_income'] = df_median['zipcode, median_income'].str[8:].apply(lambda x: x.replace(\",\",\"\"))\n",
    "df_median.drop(['zipcode, median_income'], 1, inplace=True)\n",
    "df_median.drop([11009, 13831, 16723, 16728, 16829, 17182, 17214, 17513], inplace=True) #get rid of duplicates in the df_median dataset\n",
    "df = df.merge(df_median, how='left', on='zip')\n",
    "df.ix[(df.median_income == '') | (df.median_income == ' '), 'median_income'] = np.nan\n",
    "df.median_income = df.median_income.astype(float)\n",
    "df['median_income'].fillna(value='51939', inplace=True) #approximate median income in the United States\n",
    "df['median_income'] = df['median_income'].astype(float)\n",
    "df.drop(['zip', 'Applicant City'], 1, inplace=True)# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df.pop('target')\n",
    "ind_holdout = y[y.isnull()].index\n",
    "ind_train = y[y.notnull()].index\n",
    "\n",
    "# the following is to help with the interpretation of the features in the feature importances plot\n",
    "print df.info()\n",
    "num = 266\n",
    "print \"median_income: {0}\".format(num)\n",
    "num+=-1\n",
    "print \"mean_income: {0}\".format(num)\n",
    "num+=-len(df.month.unique())\n",
    "print \"month: {0}\".format(num)\n",
    "num+=-len(df.day_of_week.unique())\n",
    "print \"day_of_week: {0}\".format(num)\n",
    "num+=-len(df.Smoker.unique())\n",
    "print \"Smoker: {0}\".format(num)\n",
    "num+=-len(df['Neustar Result Code'].unique())\n",
    "print \"Neustar Result Code: {0}\".format(num)\n",
    "num+=-len(df['Lead Source'].unique())\n",
    "print \"Lead Source: {0}\".format(num)\n",
    "num+=-len(df.Gender.unique())\n",
    "print \"Gender: {0}\".format(num)\n",
    "num+=-1\n",
    "print \"Emails: {0}\".format(num)\n",
    "num+=-1\n",
    "print \"Birthdate: {0}\".format(num)\n",
    "num+=-len(df['Applicant State/Province'].unique())\n",
    "print \"Applicant State/Province: {0}\".format(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(categorical_features = [0, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "X = enc.fit_transform(df).toarray()\n",
    "\n",
    "X_test = pd.DataFrame(X).iloc[ind_holdout]\n",
    "X_train = pd.DataFrame(X).iloc[ind_train]\n",
    "y_train = y.iloc[ind_train]\n",
    "\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As you probably guessed, this takes a while to run\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "param_dict = {'n_estimators': [10, 25, 45, 55],\n",
    "    'max_features': ['auto', 25, 35, 50]}\n",
    "gsCV_rf = GridSearchCV(rf, param_dict, n_jobs = -1, scoring='roc_auc')\n",
    "gsCV_rf.fit(X_train, y_train)\n",
    "\n",
    "print gsCV_rf.best_params_\n",
    "print gsCV_rf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predictions and Appending to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdout = pd.read_csv('/Users/brenthowe/datascience/data sets/svg/holdout_.csv')\n",
    "holdout['prediction_proba'] = gsCV_rf.predict_proba(X_test)[:,1] # generates predictions here\n",
    "holdout.to_csv('/Users/brenthowe/datascience/data sets/svg/holdout_predict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature importance plot (I don't know of a way of getting this from the gsCV_rf object from above, so I estimate again using the best parameters from the grid search)\n",
    "rf = RandomForestClassifier(n_estimators=55, max_features=25)\n",
    "rf.fit(X_train, y_train)\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(15), importances[indices[:15]], color=\"r\", yerr=std[indices[:15]], align=\"center\")\n",
    "plt.xticks(range(15), indices[:15])\n",
    "plt.xlim([-1, 15])\n",
    "# plt.show()\n",
    "plt.savefig('feature_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature numbers 250 and 251 Neustar Result Codes (I don't know what this variable corresponds to). Feature 263 and 264 are months (I don't know exactly how it was encoded but I think 263 is October and 264 is November). Features 265 and 266 are mean and median income (by zip code), respectively. Finally, feature 52 corresponds to age (i.e., Birthdate), in days. From this plot we see that the months and incomes variables are consistently influential in the classifier making good predictions. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
