{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../hackerrank_data/AWEmployees.csv')\n",
    "# print df.head()\n",
    "# print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df.pop('SalariedFlag').apply(lambda x: 1 if x==True else 0) #create the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(df.ModifiedDate .unique()) #this code helped me understand which of the original variables had no variation and varied completely...I dropped these variables\n",
    "df.drop(['BusinessEntityID', 'NationalIDNumber', 'LoginID', 'OrganizationNode', 'CurrentFlag', 'rowguid', 'ModifiedDate'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.BirthDate = pd.to_datetime('2016-12-17') - pd.to_datetime(df.BirthDate) #I converted the dates into panda datetime variables, and then took the difference between it and today's date in order to get a continuous variable\n",
    "df.HireDate = pd.to_datetime('2016-12-17') - pd.to_datetime(df.HireDate)\n",
    "\n",
    "df.BirthDate = (df.BirthDate / np.timedelta64(1, 'D')).astype(int) #I convert the timedelta variable into an int\n",
    "df.HireDate = (df.HireDate / np.timedelta64(1, 'D')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I create dummies out of the categorical variables\n",
    "df_en = pd.concat([df, pd.get_dummies(df['OrganizationLevel'], drop_first=True), pd.get_dummies(df['JobTitle'], drop_first=True), pd.get_dummies(df['MaritalStatus'], drop_first=True), pd.get_dummies(df['Gender'], drop_first=True)], axis=1)\n",
    "df_en.drop(['OrganizationLevel', 'JobTitle', 'MaritalStatus', 'Gender'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_en # Define my matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 23) #Create training and test subsamples\n",
    "# print X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Elastic Net\n",
    "The first method I use is logistic regression. The two hyperparameters I tune are (1) lambda (the coefficient on the penalty term) and (2) the elastic net mixing parameter. I also normalize the data since this algorithm uses a distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the scoring metric...I use accuracy (ratio of correct predictions to total observations). The built-in scorer is R^2, but I want the training to be done in terms of accuracy.\n",
    "def scorer(y_actual, y_pred):\n",
    "    return np.mean(np.where(y_actual==np.where(y_pred>.5, 1, 0), 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 7.5e-06, 'l1_ratio': 0}\n",
      "0.92118226601\n",
      "\n",
      "Elastic Net accuracy on test data: 0.942528735632\n"
     ]
    }
   ],
   "source": [
    "en = ElasticNet(normalize=True)\n",
    "\n",
    "score = make_scorer(scorer, greater_is_better=True)\n",
    "\n",
    "param_dict = {'alpha': [0.0000000001, 0.0000005, 0.0000075, 0.000001, 0.000005, 0.0001, 0.001, 0.01, 0.1, .25, .5, .75, 1],\n",
    "    'l1_ratio': [0, 0.0002, .45, .475, .5, .525, .55, .95, 1]}\n",
    "gsCV_en = GridSearchCV(en, param_dict, n_jobs = -1, scoring=score)\n",
    "gsCV_en.fit(X_train, y_train)\n",
    "\n",
    "print gsCV_en.best_params_\n",
    "print gsCV_en.best_score_\n",
    "\n",
    "y_pred = gsCV_en.predict(X_test)\n",
    "print '\\nElastic Net accuracy on test data: {}'.format(scorer(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal 'l1_ratio' parameter is 0 which maybe means the penalty (represented by alpha) is just too strong for the lasso dimension of the regression so all of the coefficients are getting zeroed. This would be something to explore further with more time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Gradient Boosting\n",
    "The second classifier I use is gradient boost. The two hyperparameters I tune are the learning rate (step size) and the number of estimators (number of stumps). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'learning_rate': 0.25}\n",
      "0.916256157635\n",
      "\n",
      "Gradient Boost accuracy on test data: 0.988505747126\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "param_dict = {'learning_rate': [0.00001, 0.00005, 0.0001, 0.001, 0.01, 0.1, .25, .5, .75, 1],\n",
    "    'n_estimators': [50, 75, 100, 500, 1000]}\n",
    "gsCV_gbc = GridSearchCV(gbc, param_dict, n_jobs = -1, scoring=score)\n",
    "gsCV_gbc.fit(X_train, y_train)\n",
    "\n",
    "print gsCV_gbc.best_params_\n",
    "print gsCV_gbc.best_score_\n",
    "\n",
    "y_pred = gsCV_gbc.predict(X_test)\n",
    "print '\\nGradient Boost accuracy on test data: {}'.format(scorer(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal step size is 0.25 and the optimal number of iterations is 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations and Conclusions \n",
    "For both methods I performed a grid search with k-fold (k=3 in this case) cross-validation in order to tune the hyperparamters for each method. The levels of the hyperparameters that scored the best were an improvement from the baseline model since I included the baseline parameters in the search. The measure of performance I used was accuracy. I did notice that only about 18% of the observations were such that the salaryflag variable was equal to false. It might be worthwhile to consider other scoring metrics (if I only predicted True, then my accuracy would be 82%...in any case, the model was ostensibly an improvement on this baseline). \n",
    "\n",
    "In terms of overfitting, the k-folds cross-validation technique helps mitigate this potential problem. Likewise, the two approaches I use also help mitigate overfitting (the elasticnet imposes a penalty on the coefficients and the gradient boost, as an ensemble method, tends to be less prone to overfitting).  \n",
    "\n",
    "After tuning the hyperparamters using the training set through cross-validation, I use the testing subsample to compare the two models (again, using accuracy as a measure of performance) and give an estimate of how the models would perform on a different, indepedent dataset taken from the same population. Based on these scores, the gradient boosting models is superior. \n",
    "\n",
    "Finally, below I train the entire dataset using the gradient boosting model with the best hyperparameters identified above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.25, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators = 500, learning_rate = 0.25)\n",
    "gbc.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
